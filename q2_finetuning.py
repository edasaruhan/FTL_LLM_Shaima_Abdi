# -*- coding: utf-8 -*-
"""Q2_finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hhhg4OUAcxxZ0lo8ELdk8US7wWnhLK_1
"""

pip install pandas numpy matplotlib seaborn nltk transformers

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

MODEL = f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

import pandas as pd
import random

def generate_review(sentiment):
  if sentiment == 'positive':
    adjectives = ['great', 'excellent', 'wonderful', 'fantastic', 'amazing']
  elif sentiment == 'negative':
    adjectives = ['bad', 'terrible', 'awful', 'poor', 'horrible']
  else:
    adjectives = ['ok', 'average', 'decent', 'meh', 'so-so']

  review = f"The course was {random.choice(adjectives)}. I really liked/disliked the instructor."
  return review

def create_dataset(size):
  data = []
  for _ in range(size):
    sentiment = random.choice(['positive', 'negative', 'neutral'])
    review = generate_review(sentiment)
    data.append({'text': review, 'sentiment': sentiment})
  return pd.DataFrame(data)

# Create a dataset with 100 samples
dataset = create_dataset(100)
dataset.to_csv('synthetic_reviews.csv', index=False)

import pandas as pd

# Load the generated CSV file
data = pd.read_csv('synthetic_reviews.csv')

def create_dataset(size):
  data = []
  for _ in range(size):
    sentiment = random.choice(['positive', 'negative', 'neutral'])
    review = generate_review(sentiment)
    data.append({'text': review, 'sentiment': sentiment})
  return pd.DataFrame(data)

# Create a dataset with 100 samples
dataset = create_dataset(100)

# Print the first few rows
print(dataset.head())

# Print the distribution of sentiments
print(dataset['sentiment'].value_counts())

!pip install nltk stopwords

import nltk
from nltk.corpus import stopwords

# Download stopwords if not already downloaded
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
  # Lowercase
  text = text.lower()
  # Tokenization
  words = nltk.word_tokenize(text)
  # Remove stop words
  words = [word for word in words if word not in stop_words]
  # Remove punctuation (you can customize this)
  words = [word for word in words if word.isalnum()]
  # Join words back into a string
  return ' '.join(words)

# Apply preprocessing to the text column
data['preprocessed_text'] = data['text'].apply(preprocess_text)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# Create TF-IDF vectors (for demonstration purposes)
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(data['preprocessed_text'])
y = data['sentiment']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load the pre-trained BERT model for sentiment analysis
model_name = "bert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

from transformers import Trainer, TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Define the training dataset
train_dataset = dataset.train_test_split(train_size=0.8, test_size=0.2, seed=42)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples["preprocessed_text"], padding="max_length", truncation=True)

tokenized_datasets = train_dataset.map(tokenize_function, batched=True)

# Create a trainer instance
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
)

# Train the model
trainer.train()

# Evaluate the model
results = trainer.evaluate()
print(results)

